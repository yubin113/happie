import os
from dotenv import load_dotenv
from tavily import TavilyClient
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")  # Tavily API í‚¤
API_KEY = os.environ.get('API_KEY')  # OpenAI API í‚¤

# ê²€ìƒ‰ì–´ ìµœì í™” LLM
llm = ChatOpenAI(model="gpt-3.5-turbo", openai_api_key=API_KEY)

if not TAVILY_API_KEY:
    raise ValueError("TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

def tavily_search(query, num_results=2):
    """Tavily APIë¥¼ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰ (ì„œìš¸ì‚¼ì„±ë³‘ì› ê´€ë ¨ ì •ë³´ë§Œ ê²€ìƒ‰)"""
    client = TavilyClient(api_key=TAVILY_API_KEY)

    # í•­ìƒ "ì„œìš¸ì‚¼ì„±ë³‘ì›"ì„ í¬í•¨í•˜ì—¬ ê²€ìƒ‰
    full_query = f"ì„œìš¸ì‚¼ì„±ë³‘ì› {query}"

    # ê²€ìƒ‰ ì‹¤í–‰
    response = client.search(
        query=full_query,  
        search_depth="advanced",
        max_results=num_results
    )

    # ğŸ” API ì‘ë‹µ êµ¬ì¡° í™•ì¸
    # print("ğŸ“¡ API ì‘ë‹µ ë°ì´í„°:", response)

    # ê²°ê³¼ ì •ë¦¬
    if not isinstance(response, dict):
        return "âŒ ì˜ëª»ëœ API ì‘ë‹µ í˜•ì‹ì…ë‹ˆë‹¤."

    results = response.get("results", [])
    if not results:
        return "âŒ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    formatted_results = []
    
    for item in results:
        title = item.get("title", "ì œëª© ì—†ìŒ")
        url = item.get("link") or item.get("url", "URL ì—†ìŒ")  # `link`ì™€ `url` ì¤‘ ì¡´ì¬í•˜ëŠ” ê²ƒ ì‚¬ìš©
        snippet = item.get("content", "ë‚´ìš© ì—†ìŒ")
        
        # formatted_results.append(f"ğŸ“Œ **{title}**\nğŸ”— {url}\nğŸ“ {snippet[:400]}...")  # ë‚´ìš© ì¼ë¶€ë§Œ í‘œì‹œ
        formatted_results.append(snippet[:400])

    return formatted_results

# âœ… í…ŒìŠ¤íŠ¸

def optimize_query(user_query):
    """ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´ë¥¼ ë” ì¢‹ì€ ê²€ìƒ‰ì–´ë¡œ ë°˜í™˜"""
    prompt = PromptTemplate(
        input_variables=["query"],
        template="'{query}'ì„(ë¥¼) ë” ì •í™•í•œ ê²€ìƒ‰ì–´ë¡œ ë°”ê¿”ì¤˜. ë‹¨, ë³€í™˜ëœ ê²€ìƒ‰ì–´ë§Œ ì¶œë ¥í•˜ê³  ë¶ˆí•„ìš”í•œ ì„¤ëª…ì€ ì“°ì§€ ë§ˆ."
    )
    optimized_query = llm.predict(prompt.format(query=user_query))
    return optimized_query

# ì˜ˆì œ ì‹¤í–‰
# user_input = "ì‹¬ì¥ë‚´ê³¼ ìµœê·¼ ë…¼ë¬¸ ë­ ìˆëŠ”ì§€ ì•Œë ¤ì¤˜"
# optimized_query = optimize_query(user_input)
# print("ìµœì í™”ëœ ê²€ìƒ‰ì–´:", optimized_query)
# result = tavily_search(optimized_query, num_results=2)
# print("tavily api ê²€ìƒ‰ê²°ê³¼: " , result)
