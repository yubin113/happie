import sys
sys.path.append(r"C:\Users\SSAFY\Desktop\S12P21E103\ROS\speech")
from search_chromadb import search_hospital_info
import logging
import openai
from dotenv import load_dotenv
import os
from hospital_google_search import google_search
from tavily_search import tavily_search, optimize_query
import re

# load .env
load_dotenv()

API_KEY = os.environ.get('API_KEY')

# ğŸ”¹ ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG, format="%(asctime)s [%(levelname)s] %(message)s")

# OpenAI API í‚¤ ì„¤ì •
client = openai.OpenAI(api_key=API_KEY)


# ğŸ”¹ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
history = []


# ëŒ€í™”í˜• ì±—ë´‡ ìƒì„± í•¨ìˆ˜
def generate_response(query, search_results, external_search):
    logging.debug(f"ì‚¬ìš©ì ì§ˆë¬¸ ìˆ˜ì‹ : {query}")
    logging.debug(f"ê²€ìƒ‰ëœ ë³‘ì› ì •ë³´ ì›ë³¸: {search_results}")
    
    # ê²€ìƒ‰ëœ ë³‘ì› ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ìš”ì•½
    summarized_results = [
        f"{item.get('facility_name', 'ì•Œ ìˆ˜ ì—†ìŒ')}: {item.get('floor_info', 'ì •ë³´ ì—†ìŒ')} | {item.get('location', 'ì •ë³´ ì—†ìŒ')} | {item.get('service_description', 'ì •ë³´ ì—†ìŒ')}"
        for item in search_results[:5]
    ]
    search_results_str = "\n".join(summarized_results)

    messages=[
            {
                "role": "system",
                "content": f"""
                    ë„ˆëŠ” ì‚¼ì„±ë³‘ì›ì˜ ì˜ë£Œ ì‹œì„¤ ì •ë³´ë¥¼ ì•ˆë‚´í•˜ê³  ì¼ìƒëŒ€í™”ë„ ê°€ëŠ¥í•œ AIì±—ë´‡ì´ì´ì•¼.
                    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì´í•´í•˜ê³ , ê²€ìƒ‰ëœ ë³‘ì› ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ì œê³µí•´.

                    í˜„ì¬ ì œê³µí•  ìˆ˜ ìˆëŠ” ë³‘ì› ì •ë³´:
                    {search_results_str if search_results else "í˜„ì¬ ì œê³µí•  ìˆ˜ ìˆëŠ” ë³‘ì› ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."}

                    ì™¸ë¶€ ê²€ìƒ‰ ì •ë³´: 
                    {external_search if external_search else "í˜„ì¬ ì œê³µí•  ìˆ˜ ìˆëŠ” ì™¸ë¶€ ê²€ìƒ‰ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."}

                    ì‘ë‹µ ê·œì¹™:
                    1. **ì§ˆë¬¸ì„ ë¶„ì„**í•´ì„œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì°¾ì•„.
                    2. **ê²€ìƒ‰ëœ ì •ë³´ê°€ ìˆìœ¼ë©´**, í•´ë‹¹ ë‚´ìš©ì„ ì •í™•í•œ ë†’ì„ë§ë¡œ ì „ë‹¬í•´.
                       - ì˜ˆ: "ì‘ê¸‰ì‹¤ì€ 1ì¸µì— ìˆìŠµë‹ˆë‹¤."
                    3. **ê²€ìƒ‰ëœ ì •ë³´ê°€ ì—†ìœ¼ë©´**, `"í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."`ë¼ê³  ë‹µí•˜ê±°ë‚˜ ë„ˆê°€ ë‹µí•  ìˆ˜ ìˆëŠ”ê±°ë©´ ë‹µí•´ì£¼ê±°ë‚˜ ì¶”ê°€ ì§ˆë¬¸ì„ ìœ ë„í•´.
                    4. **ì‚¬ìš©ìê°€ ì˜ëª» ì•Œê³  ìˆì„ ê²½ìš°**, ì˜¬ë°”ë¥¸ ì •ë³´ë¥¼ ì œê³µí•´.
                       - ì˜ˆ: "ì‘ê¸‰ì‹¤ì€ 2ì¸µì´ ì•„ë‹ˆë¼ 1ì¸µì— ìˆìŠµë‹ˆë‹¤."
                    5. **ì‘ë‹µì€ í•­ìƒ ë†’ì„ë§ë¡œ ì‘ì„±**í•´.

                    ë‹µë³€ë§Œ ì¶œë ¥í•´.
                """
            },
            # {
            #     "role": "user",
            #     "content": query
            # }
        ]

    # ğŸ”¹ ê¸°ì¡´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
    messages.extend(history)

    # ğŸ”¹ í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
    messages.append({"role": "user", "content": query})


    logging.info(f"ë©”ì‹œì§€ ì¶œë ¥ : {messages}")

    # ğŸ”¹ OpenAI GPT ëª¨ë¸ í˜¸ì¶œ
    logging.info("GPT ëª¨ë¸ì„ ì‚¬ìš©í•´ ì‘ë‹µ ìƒì„± ì¤‘...")
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",  # ë˜ëŠ” "gpt-3.5-turbo"
        messages=messages,
        max_tokens=150,
        temperature=0.7
    )

    print(f"ì‘ë‹µ: {response}")
    response_text = response.choices[0].message.content

    # âœ… ìƒˆë¡œìš´ ëŒ€í™” ë‚´ì—­ ì €ì¥
    history.append({"role": "user", "content": query})
    history.append({"role": "assistant", "content": response_text})


    response_text = response.choices[0].message.content

    logging.debug(f"ìµœì¢… ì‘ë‹µ: {response_text}")
    return response_text

# history ì´ˆê¸°í™” í•¨ìˆ˜
def clear_history():
    global history
    history = []
    print("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ğŸ”¹ ëŒ€í™”í˜• ì±—ë´‡ ì‹¤í–‰ í•¨ìˆ˜
def chat():
    print("ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'exit'ì„ ì…ë ¥í•˜ì„¸ìš”.")

    while True:
        user_input = input("ì‚¬ìš©ì: ")
        logging.info(f"ì‚¬ìš©ì ì…ë ¥: {user_input}")

        if user_input.lower() == 'exit':
            logging.info("ì±—ë´‡ ì¢…ë£Œ")
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ê¸°
        search_results = search_hospital_info(user_input)
        logging.debug(f"ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜: {search_results}")
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ë‹¤ë©´ ê²½ê³  ë©”ì‹œì§€
        # if not search_results:
        #     logging.warning("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        #     print("âŒ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        #     continue

        #ì™¸ë¶€ ê²€ìƒ‰
        external_search= []
    
        # ìµœì í™”ëœ ê²€ìƒ‰ì–´ ìƒã…‡ì„±
        optimized_query = optimize_query(user_input)
        logging.info("ìµœì í™”ëœ ê²€ìƒ‰ì–´: ", optimized_query)

        # Google ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        google_results = google_search("site:samsunghospital.com " , optimized_query)
        if google_results:
            logging.info("Google ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€")
            external_search.extend(google_results)  # ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ì— ì¶”ê°€
            print("ğŸ” Google ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
        
        # tavily_search_answer = tavily_search(optimized_query)
        # if tavily_search_answer:
        #     logging.info("tavily ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€")
        #     external_search.extend(tavily_search_answer)

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì‘ë‹µ ìƒì„±
        print(search_results)
        print("ì™¸ë¶€ ê²€ìƒ‰ ê²°ê³¼: " , external_search)
        response = generate_response(user_input, search_results, external_search)




if __name__ == "__main__":
    chat()