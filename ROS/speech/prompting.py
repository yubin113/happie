import sys
sys.path.append(r"C:\Users\SSAFY\Desktop\S12P21E103\ROS\speech")
from search_vector import search_hospital_info
from llama_cpp import Llama
import logging
# from memory_manager import ConversationMemoryManager  # memory_manager.pyì—ì„œ ì •ì˜í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸


# ğŸ”¹ ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG, format="%(asctime)s [%(levelname)s] %(message)s")

# Llama ëª¨ë¸ ë¡œë”©
model_path = r"C:\Users\SSAFY\Desktop\LLM\llama-3-Korean-Bllossom-8B.Q8_0.gguf"
logging.info(f"ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
llama = Llama(model_path=model_path)
logging.info("ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

### ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# memory_manager = ConversationMemoryManager()

# ëŒ€í™”í˜• ì±—ë´‡ ìƒì„± í•¨ìˆ˜
def generate_response(query, search_results):
    logging.debug(f"ì‚¬ìš©ì ì§ˆë¬¸ ìˆ˜ì‹ : {query}")
    logging.debug(f"ê²€ìƒ‰ëœ ë³‘ì› ì •ë³´ ì›ë³¸: {search_results}")
    
    # ê²€ìƒ‰ëœ ë³‘ì› ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ìš”ì•½
    summarized_results = [
        f"{item.get('facility_name', 'ì•Œ ìˆ˜ ì—†ìŒ')}: {item.get('floor_info', 'ì •ë³´ ì—†ìŒ')} | {item.get('location', 'ì •ë³´ ì—†ìŒ')} | {item.get('service_description', 'ì •ë³´ ì—†ìŒ')}"
        for item in search_results[:3]  # ìµœëŒ€ 3ê°œ ê²°ê³¼ë§Œ ìš”ì•½
    ]
    search_results_str = "\n".join(summarized_results)
    
    ### ì´ì „ ëŒ€í™” ì´ë ¥ ê°€ì ¸ì˜¤ê¸°
    # previous_conversations = memory_manager.get_previous_conversations()
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
    prompt_template = """
    ë„ˆëŠ” ì‚¼ì„±ë³‘ì›ì˜ ì˜ë£Œ ì‹œì„¤ ì •ë³´ë¥¼ ì•ˆë‚´í•˜ëŠ” AIì•¼. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì´í•´í•˜ê³ , ê²€ìƒ‰ëœ ë³‘ì› ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ì œê³µí•´.
    
    ### ì§ˆë¬¸:
    {user_query}  

    ### ê²€ìƒ‰ëœ ë³‘ì› ì •ë³´:
    {search_results}  

    ---

    ì‘ë‹µ ê·œì¹™:  
    1. **ì§ˆë¬¸ì„ ë¶„ì„**í•´ì„œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì°¾ì•„.  
    2. **ê²€ìƒ‰ëœ ì •ë³´ê°€ ìˆìœ¼ë©´**, í•´ë‹¹ ë‚´ìš©ì„ ì •í™•í•œ ë†’ì„ë§ë¡œ ì „ë‹¬í•´.  
    - ì˜ˆ: "ì‘ê¸‰ì‹¤ì€ 1ì¸µì— ìˆìŠµë‹ˆë‹¤."  
    3. **ê²€ìƒ‰ëœ ì •ë³´ê°€ ì—†ìœ¼ë©´**, `"í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."`ë¼ê³  ë‹µí•˜ê³ , ì¶”ê°€ ì§ˆë¬¸ì„ ìœ ë„í•´.  
    4. **ì‚¬ìš©ìê°€ ì˜ëª» ì•Œê³  ìˆì„ ê²½ìš°**, ì˜¬ë°”ë¥¸ ì •ë³´ë¥¼ ì œê³µí•´.  
    - ì˜ˆ: "ì‘ê¸‰ì‹¤ì€ 2ì¸µì´ ì•„ë‹ˆë¼ 1ì¸µì— ìˆìŠµë‹ˆë‹¤."  
    5. **ì‘ë‹µì€ í•­ìƒ ë†’ì„ë§ë¡œ ì‘ì„±**í•´.  

    ë‹µë³€ë§Œ ì¶œë ¥í•´.  
    """


    # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
    prompt = prompt_template.format(
        # previous_conversations=previous_conversations,
        user_query=query,
        search_results=search_results_str
    )

    logging.debug(f"í”„ë¡¬í”„íŠ¸ ìƒì„±:\n{prompt}")

    # ğŸ”¹ Llama ëª¨ë¸ ì‹¤í–‰í•˜ì—¬ ì‘ë‹µ ìƒì„±
    logging.info("Llama ëª¨ë¸ì„ ì‚¬ìš©í•´ ì‘ë‹µ ìƒì„± ì¤‘...")
    response = llama(prompt, max_tokens=50, temperature=0.2)
    logging.info(f"ì‘ë‹µ ìƒì„± ì™„ë£Œ.")

    # ğŸ”¹ ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ê³ , ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
    response_text = response.get('choices', [{}])[0].get('text', 'No response generated.').strip()
    response_text = response_text.replace("ì±—ë´‡: ", "", 1).strip()
    
    # ğŸ”¹ í›„ì²˜ë¦¬: ë’¤ì—ì„œë¶€í„° ê°€ì¥ ê°€ê¹Œìš´ ë§ˆì¹¨í‘œê¹Œì§€ ìœ ì§€
    last_period_index = response_text.rfind(".")
    if last_period_index != -1:  # ë§ˆì¹¨í‘œê°€ ìˆìœ¼ë©´ í•´ë‹¹ ë¶€ë¶„ê¹Œì§€ë§Œ ë‚¨ê¹€
        response_text = response_text[: last_period_index + 1]

    logging.debug(f"ìµœì¢… ì‘ë‹µ: {response_text}")
    return response_text


# ğŸ”¹ ëŒ€í™”í˜• ì±—ë´‡ ì‹¤í–‰ í•¨ìˆ˜
def chat():
    print("ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'exit'ì„ ì…ë ¥í•˜ì„¸ìš”.")

    while True:
        user_input = input("ì‚¬ìš©ì: ")
        logging.info(f"ì‚¬ìš©ì ì…ë ¥: {user_input}")

        if user_input.lower() == 'exit':
            logging.info("ì±—ë´‡ ì¢…ë£Œ")
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ê¸°
        search_results = search_hospital_info(user_input)
        logging.debug(f"ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜: {search_results}")
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ë‹¤ë©´ ê²½ê³  ë©”ì‹œì§€
        if not search_results:
            logging.warning("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            print("âŒ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì‘ë‹µ ìƒì„±
        generate_response(user_input, search_results)
        # response = generate_response(user_input, search_results)
        # print(f"ì±—ë´‡: {response} ì´ìƒ ë!")


if __name__ == "__main__":
    chat()
